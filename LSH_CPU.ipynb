{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minht\\AppData\\Local\\Temp\\ipykernel_23340\\2877273640.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('movies_with_test.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movies_with_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(930739, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(text, min_length=1, max_length=None):\n",
    "  tags = [tag.strip() for tag in text.split(',')]  # Tách các tag riêng biệt\n",
    "\n",
    "  all_shingles = set()\n",
    "  for tag in tags:\n",
    "      words = tag.split()  # Tách tag thành các từ\n",
    "      if max_length is None:\n",
    "          max_length = max(len(word) for word in words)\n",
    "      for word in words:\n",
    "          for i in range(min_length, min(max_length, len(word)) + 1):\n",
    "              all_shingles.add(word[:i])\n",
    "  return all_shingles\n",
    "\n",
    "df['shingles'] = df['genres'].astype(str).apply(create_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         {Comed, Co, Come, Com, C, Comedy}\n",
       "1                                 {Dra, Dram, Drama, D, Dr}\n",
       "2         {Comed, Co, Adu, Come, Com, Ad, C, Adul, Adult...\n",
       "3         {Anim, Fam, Animation, Animati, Fa, Animatio, ...\n",
       "4                                 {Dra, Dram, Drama, D, Dr}\n",
       "                                ...                        \n",
       "930734                                         {n, nan, na}\n",
       "930735                                         {n, nan, na}\n",
       "930736                                         {n, nan, na}\n",
       "930737                                         {n, nan, na}\n",
       "930738                                         {n, nan, na}\n",
       "Name: shingles, Length: 930739, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['shingles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_functions(num_hash_functions, max_val):\n",
    "    hash_functions = []\n",
    "    for _ in range(num_hash_functions):\n",
    "        a = random.randint(1, max_val)\n",
    "        b = random.randint(0, max_val)\n",
    "        hash_functions.append(lambda x, a=a, b=b: (a * x + b) % max_val)\n",
    "    return hash_functions\n",
    "\n",
    "def minhash_signature(shingles, hash_functions):\n",
    "    signature = [float('inf')] * len(hash_functions) \n",
    "    for shingle in shingles:\n",
    "        shingle_hash = hash(shingle)\n",
    "        for i, hash_func in enumerate(hash_functions):\n",
    "            signature[i] = min(signature[i], hash_func(shingle_hash))\n",
    "    return signature\n",
    "\n",
    "all_shingles = set()\n",
    "for shingles_list in df['shingles']:\n",
    "    all_shingles.update(shingles_list)\n",
    "# Create 10 hash functions\n",
    "hash_functions = create_hash_functions(10, len(all_shingles))\n",
    "\n",
    "# Apply minhash_signature to each row in the `shingles` column\n",
    "df['minhash_signature'] = df['shingles'].apply(lambda x: minhash_signature(x, hash_functions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A',\n",
       " 'Ac',\n",
       " 'Act',\n",
       " 'Acti',\n",
       " 'Actio',\n",
       " 'Action',\n",
       " 'Ad',\n",
       " 'Adu',\n",
       " 'Adul',\n",
       " 'Adult',\n",
       " 'Adv',\n",
       " 'Adve',\n",
       " 'Adven',\n",
       " 'Advent',\n",
       " 'Adventu',\n",
       " 'Adventur',\n",
       " 'Adventure',\n",
       " 'An',\n",
       " 'Ani',\n",
       " 'Anim',\n",
       " 'Anima',\n",
       " 'Animat',\n",
       " 'Animati',\n",
       " 'Animatio',\n",
       " 'Animation',\n",
       " 'B',\n",
       " 'Bi',\n",
       " 'Bio',\n",
       " 'Biog',\n",
       " 'Biogr',\n",
       " 'Biogra',\n",
       " 'Biograp',\n",
       " 'Biograph',\n",
       " 'Biography',\n",
       " 'C',\n",
       " 'Co',\n",
       " 'Com',\n",
       " 'Come',\n",
       " 'Comed',\n",
       " 'Comedy',\n",
       " 'Cr',\n",
       " 'Cri',\n",
       " 'Crim',\n",
       " 'Crime',\n",
       " 'D',\n",
       " 'Do',\n",
       " 'Doc',\n",
       " 'Docu',\n",
       " 'Docum',\n",
       " 'Docume',\n",
       " 'Documen',\n",
       " 'Document',\n",
       " 'Documenta',\n",
       " 'Documentar',\n",
       " 'Documentary',\n",
       " 'Dr',\n",
       " 'Dra',\n",
       " 'Dram',\n",
       " 'Drama',\n",
       " 'F',\n",
       " 'Fa',\n",
       " 'Fam',\n",
       " 'Fami',\n",
       " 'Famil',\n",
       " 'Family',\n",
       " 'Fan',\n",
       " 'Fant',\n",
       " 'Fanta',\n",
       " 'Fantas',\n",
       " 'Fantasy',\n",
       " 'G',\n",
       " 'Ga',\n",
       " 'Gam',\n",
       " 'Game',\n",
       " 'Game-',\n",
       " 'Game-S',\n",
       " 'Game-Sh',\n",
       " 'Game-Sho',\n",
       " 'Game-Show',\n",
       " 'H',\n",
       " 'Hi',\n",
       " 'His',\n",
       " 'Hist',\n",
       " 'Histo',\n",
       " 'Histor',\n",
       " 'History',\n",
       " 'Ho',\n",
       " 'Hor',\n",
       " 'Horr',\n",
       " 'Horro',\n",
       " 'Horror',\n",
       " 'M',\n",
       " 'Mu',\n",
       " 'Mus',\n",
       " 'Musi',\n",
       " 'Music',\n",
       " 'Musica',\n",
       " 'Musical',\n",
       " 'My',\n",
       " 'Mys',\n",
       " 'Myst',\n",
       " 'Myste',\n",
       " 'Myster',\n",
       " 'N',\n",
       " 'Ne',\n",
       " 'New',\n",
       " 'News',\n",
       " 'R',\n",
       " 'Re',\n",
       " 'Rea',\n",
       " 'Real',\n",
       " 'Reali',\n",
       " 'Realit',\n",
       " 'Reality',\n",
       " 'Reality-',\n",
       " 'Reality-T',\n",
       " 'Ro',\n",
       " 'Rom',\n",
       " 'Roma',\n",
       " 'Roman',\n",
       " 'Romanc',\n",
       " 'Romance',\n",
       " 'S',\n",
       " 'Sc',\n",
       " 'Sci',\n",
       " 'Sci-',\n",
       " 'Sci-F',\n",
       " 'Sci-Fi',\n",
       " 'Sh',\n",
       " 'Sho',\n",
       " 'Shor',\n",
       " 'Short',\n",
       " 'Sp',\n",
       " 'Spo',\n",
       " 'Spor',\n",
       " 'Sport',\n",
       " 'T',\n",
       " 'Ta',\n",
       " 'Tal',\n",
       " 'Talk',\n",
       " 'Talk-',\n",
       " 'Talk-S',\n",
       " 'Talk-Sh',\n",
       " 'Talk-Sho',\n",
       " 'Talk-Show',\n",
       " 'Th',\n",
       " 'Thr',\n",
       " 'Thri',\n",
       " 'Thril',\n",
       " 'Thrill',\n",
       " 'Thrille',\n",
       " 'Thriller',\n",
       " 'W',\n",
       " 'Wa',\n",
       " 'War',\n",
       " 'We',\n",
       " 'Wes',\n",
       " 'West',\n",
       " 'Weste',\n",
       " 'Wester',\n",
       " 'Western',\n",
       " 'n',\n",
       " 'na',\n",
       " 'nan'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, num_bands, num_rows_per_band):\n",
    "        self.num_bands = num_bands\n",
    "        self.num_rows_per_band = num_rows_per_band\n",
    "        self.buckets = {}  # Lưu trữ các nhóm (bucket)\n",
    "\n",
    "\n",
    "    def hash_function(self, signature, band_index):\n",
    "        \"\"\"\n",
    "        Hàm băm đơn giản để ánh xạ một phần của chữ ký MinHash vào một nhóm.\n",
    "        \"\"\"\n",
    "        start_row = band_index * self.num_rows_per_band\n",
    "        end_row = start_row + self.num_rows_per_band\n",
    "        band_signature = tuple(signature[start_row:end_row])\n",
    "        return hash(band_signature)\n",
    "\n",
    "    def insert(self, key, signature):\n",
    "        \"\"\"\n",
    "        Chèn một mục (key) và chữ ký MinHash của nó vào các nhóm LSH.\n",
    "        \"\"\"\n",
    "        for band_index in range(self.num_bands):\n",
    "            bucket_id = self.hash_function(signature, band_index)\n",
    "            if bucket_id not in self.buckets:\n",
    "                self.buckets[bucket_id] = []\n",
    "            self.buckets[bucket_id].append(key)\n",
    "\n",
    "    def movie_similar(self, query_signature):\n",
    "        \"\"\"\n",
    "        Truy vấn các mục có khả năng tương tự dựa trên chữ ký MinHash của truy vấn.\n",
    "        \"\"\"\n",
    "        candidates = set()\n",
    "        for band_index in range(self.num_bands):\n",
    "            bucket_id = self.hash_function(query_signature, band_index)\n",
    "            if bucket_id in self.buckets:\n",
    "                candidates.update(self.buckets[bucket_id])\n",
    "        return candidates\n",
    "    def movie_query(self, tag_query):\n",
    "        \"\"\"\n",
    "        Truy vấn các bộ phim có khả năng tương tự dựa trên tag truy vấn.\n",
    "        \"\"\"\n",
    "\n",
    "        # Tạo tập hợp shingles từ tag truy vấn\n",
    "        query_shingles = create_shingles(tag_query)\n",
    "\n",
    "        # Chuyển đổi shingles thành MinHash signature (sử dụng hash_functions của LSH)\n",
    "        query_signature = minhash_signature(query_shingles, hash_functions)    \n",
    "\n",
    "        # Tìm các bộ phim ứng viên bằng LSH\n",
    "        candidate_movie_ids = self.movie_similar(query_signature)\n",
    "\n",
    "        return candidate_movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Tính toán độ tương đồng Jaccard giữa hai tập hợp\n",
    "    \"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The Leafblower', 'Episode dated 6 May 2005', 'War on Three Fronts', 'The Pythoness', 'Moving McAllister', 'Décembre', 'Sinfonia Amazônica', 'One Piece: Oounabara ni hirake! Dekkai dekkai chichi no yume!', 'Macao', 'Diahann Carroll: No Strings', 'Alas sobre El Chaco', 'Destiny Love', 'The Millionaire', 'The Target Shoots First', 'Chapter 15', 'The Utilizer', 'The Brothers Gruff', 'The Sword and the Dragon', 'The Mystery of Edward Sims: Part 2', 'Niji no ike no jigoku janguru', 'A Fox in a Fix', 'Tobunda ôzora he', 'Synthetic Love', 'The Ixtafa Affair', 'Episode dated 10 January 2015', 'Super Dave: Daredevil for Hire'}\n"
     ]
    }
   ],
   "source": [
    "lsh = LSH(num_bands=5, num_rows_per_band=2)  # Chia chữ ký MinHash thành 5 bands, mỗi band có 2 hàng\n",
    "\n",
    "# Chèn các bộ phim vào LSH\n",
    "for index, row in df.iterrows():\n",
    "    lsh.insert(row['originalTitle'], row['minhash_signature'])\n",
    "\n",
    "# Truy vấn phim tương tự\n",
    "query_movie = 'The Millionaire'\n",
    "query_signature = df[df['originalTitle'] == query_movie]['minhash_signature'].iloc[0]\n",
    "candidate_movies = lsh.movie_similar(query_signature)\n",
    "print(candidate_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: The Leafblower, Jaccard Similarity: 0.05454545454545454\n",
      "Movie: Episode dated 6 May 2005, Jaccard Similarity: 0.05454545454545454\n",
      "Movie: War on Three Fronts, Jaccard Similarity: 0.045454545454545456\n",
      "Movie: The Pythoness, Jaccard Similarity: 0.045454545454545456\n",
      "Movie: Moving McAllister, Jaccard Similarity: 0.045454545454545456\n",
      "Movie: Décembre, Jaccard Similarity: 0.013333333333333332\n",
      "Movie: Sinfonia Amazônica, Jaccard Similarity: 0.06\n",
      "Movie: One Piece: Oounabara ni hirake! Dekkai dekkai chichi no yume!, Jaccard Similarity: 0.03636363636363636\n",
      "Movie: Macao, Jaccard Similarity: 0.03636363636363636\n",
      "Movie: Diahann Carroll: No Strings, Jaccard Similarity: 0.03636363636363636\n",
      "Movie: Alas sobre El Chaco, Jaccard Similarity: 0.03333333333333333\n",
      "Movie: Destiny Love, Jaccard Similarity: 0.03636363636363636\n",
      "Movie: The Target Shoots First, Jaccard Similarity: 0.06\n",
      "Movie: Chapter 15, Jaccard Similarity: 0.05\n",
      "Movie: The Utilizer, Jaccard Similarity: 0.038461538461538464\n",
      "Movie: The Brothers Gruff, Jaccard Similarity: 0.05\n",
      "Movie: The Sword and the Dragon, Jaccard Similarity: 0.05454545454545454\n",
      "Movie: The Mystery of Edward Sims: Part 2, Jaccard Similarity: 0.05\n",
      "Movie: Niji no ike no jigoku janguru, Jaccard Similarity: 0.04\n",
      "Movie: A Fox in a Fix, Jaccard Similarity: 0.06999999999999999\n",
      "Movie: Tobunda ôzora he, Jaccard Similarity: 0.04\n",
      "Movie: Synthetic Love, Jaccard Similarity: 0.03333333333333333\n",
      "Movie: The Ixtafa Affair, Jaccard Similarity: 0.05\n",
      "Movie: Episode dated 10 January 2015, Jaccard Similarity: 0.05\n",
      "Movie: Super Dave: Daredevil for Hire, Jaccard Similarity: 0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "similarities = []\n",
    "for candidate_movie in candidate_movies:\n",
    "    candidate_signature = df[df['originalTitle'] == candidate_movie]['minhash_signature'].iloc[0]\n",
    "    similarity = jaccard_similarity(set(query_signature), set(candidate_signature)) / len(hash_functions)\n",
    "    similarities.append((candidate_movie, similarity))\n",
    "\n",
    "threshold = 0.1\n",
    "filtered_movies = [movie for movie, sim in similarities if sim >= threshold]\n",
    "for movie, similarity in similarities:\n",
    "    if movie != query_movie:  # Loại trừ chính bộ phim truy vấn\n",
    "        print(f\"Movie: {movie}, Jaccard Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tit Teazer Files 1', 'Swedish Erotica 9', 'White Trash Whore 26', 'Meat My Ass 3', 'Botas Texanas y balas salvajes', 'Der Hausmeister', 'World Sex Tour 22: Jamaica'}\n"
     ]
    }
   ],
   "source": [
    "tag_query = \"Ad\"\n",
    "query_result = lsh.movie_query(tag_query)\n",
    "print(query_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
